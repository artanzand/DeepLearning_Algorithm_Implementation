{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Neural Network - Abstract Implementation\n",
    "\n",
    "Now that we have created the pieces of the model, we will import all and put them together for an overall deep neural network that is capable of learning and prediction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "import sklearn\n",
    "import sklearn.datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from dnn_utils_all import *\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "np.random.seed(2021)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "db8e38ac17a149dc8e73efffd5cfbecf",
     "grade": false,
     "grade_id": "cell-46e7e26fe75ece95",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "##  L-layer Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the helper functions implemented previously to build an $L$-layer neural network with the following structure: [LINEAR -> RELU] $\\times$ (L-1) -> LINEAR -> SIGMOID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dimension of layers\n",
    "layers_dims = [350, 20, 7, 5, 1] #  4-layer model, 350 is the number of features from the data that I will be loading later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_layer_model(X, Y, layers_dims, alpha = 0.0075, n_iter = 3000, verbose=False):\n",
    "    \"\"\"\n",
    "    Implements a L-layer neural network: [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array\n",
    "        input data of shape (num_features, number of examples)\n",
    "    Y : array\n",
    "        true label vector of shape (1, number of examples)\n",
    "    layers_dims : list \n",
    "        containing the input size and each layer size, of length (number of layers + 1).\n",
    "    alpha : float\n",
    "        learning rate of the gradient descent update rule\n",
    "    n_iter : int\n",
    "        number of iterations of the optimization loop\n",
    "    verbose : boolean\n",
    "        if True, it prints the cost every 100 steps\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    parameters -- parameters learnt by the model. They can then be used to predict.\n",
    "    \"\"\"\n",
    "\n",
    "    np.random.seed(1)\n",
    "    costs = []  # keep track of cost\n",
    "    \n",
    "    # Parameters initialization.\n",
    "    parameters = initialize_parameters_deep(layers_dims)\n",
    "    \n",
    "    # Loop (gradient descent)\n",
    "    for i in range(0, n_iter):\n",
    "\n",
    "        # Forward propagation\n",
    "        AL, caches = L_model_forward(X, parameters)\n",
    "        \n",
    "        # Compute cost\n",
    "        cost = compute_cost(AL, Y)\n",
    "    \n",
    "        # Backward propagation\n",
    "        grads = L_model_backward(AL, Y, caches)\n",
    " \n",
    "        # Update parameters\n",
    "        parameters = update_parameters(parameters, grads, alpha)\n",
    "                \n",
    "        # Print the cost every 100 iterations\n",
    "        if verbose and i % 100 == 0 or i == n_iter - 1:\n",
    "            print(\"Cost after iteration {}: {}\".format(i, np.squeeze(cost)))\n",
    "        if i % 100 == 0 or i == n_iter:\n",
    "            costs.append(cost)\n",
    "    \n",
    "    return parameters, costs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usage Example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters, costs = L_layer_model(X_train, y_train, layers_dims, n_iter=2500, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_train = predict(X_train, y_train, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test = predict(X_test, y_test, parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This whole series of coding neural network from scratch was just a demonstration of how we could implement the model from the underlying math. The next improvement could be adding regularization to the model. However, the proper way of designing a model is to include them in a Class to allow for attribute like fit and predict. This could be follow up project to this model development. "
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "neural-networks-deep-learning",
   "graded_item_id": "TSPse",
   "launcher_item_id": "24mxX"
  },
  "kernelspec": {
   "display_name": "Python [conda env:573]",
   "language": "python",
   "name": "conda-env-573-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

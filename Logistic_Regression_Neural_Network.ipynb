{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One-node Neural Net through Logistic Regression\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 1 - Problem Statement ##\n",
    "\n",
    "Given a dataset containing:  \n",
    "- a training set of m_train images labeled as True (y=1) or False (y=0)\n",
    "- a test set of m_test images labeled the same as above\n",
    "- each image is of shape (num_pixel, num_pixel, 3) where 3 is for the 3 channels (RGB). Thus, each image is square (height = num_pixel) and (width = num_pixel).\n",
    "\n",
    "we are trying to build a simple image-recognition algorithm that can correctly classify whether given pictures belong to a class or not.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Preprocessing\n",
    "\n",
    "Considering the shape of each image, our hypothetical train and test set would have a a shape of (num_examples, num_pixel, num_pixel, 3). Our first step to feed the data to the neural network would be to flatten each individual example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "921fe679a632ec7ec9963069fa405725",
     "grade": false,
     "grade_id": "cell-c4e7e9c1f174eb83",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Reshape the training and test examples\n",
    "train_set_x_flatten = train_set_x_orig.reshape(train_set_x_orig.shape[0], -1).T\n",
    "test_set_x_flatten = test_set_x_orig.reshape(test_set_x_orig.shape[0], -1).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During the training of the model, we will be multiplying weights and add biases to some initial inputs in order to observe neuron activations. It is extremely important for each feature to have a similar range such that our gradients don't explode. Knowing that the RGB values in each pixel channel range from 0 to 255, we will divide all values by 255 to standardize our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_x = train_set_x_flatten / 255.\n",
    "test_set_x = test_set_x_flatten / 255."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - The learning algorithm ##\n",
    "\n",
    "The intention is to build a Logistic Regression, using a Neural Network mindset (inspired by deeplearning.ai). See below for a graphical representation showing why Logistic Regression can be viewed as a Neural Network.\n",
    "\n",
    "<center><img src=\"model.PNG\" width=600></center>  \n",
    "    \n",
    "**Mathematical expression of the algorithm**:\n",
    "\n",
    "For one example $x^{(i)}$:\n",
    "$$z^{(i)} = w^T x^{(i)} + b $$\n",
    "$$\\hat{y}^{(i)} = a^{(i)} = sigmoid(z^{(i)})$$ \n",
    "$$ \\mathcal{L}(a^{(i)}, y^{(i)}) =  - y^{(i)}  \\log(a^{(i)}) - (1-y^{(i)} )  \\log(1-a^{(i)})$$\n",
    "\n",
    "The cost is then computed by summing over all training examples:\n",
    "$$ J = \\frac{1}{m} \\sum_{i=1}^m \\mathcal{L}(a^{(i)}, y^{(i)})$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='4'></a>\n",
    "## 4 - Building the parts of our algorithm ## \n",
    "\n",
    "The main steps for building a Neural Network are:\n",
    "1. Define the model structure (such as number of input features) \n",
    "2. Initialize the model's parameters\n",
    "3. Loop:\n",
    "    - Calculate current loss (forward propagation)\n",
    "    - Calculate current gradient (backward propagation)\n",
    "    - Update parameters (gradient descent)\n",
    "\n",
    "We will build 1-3 separately and integrate them into one function we call `model()`.\n",
    "\n",
    "<a name='4-1'></a>\n",
    "### 4.1 - Helper functions\n",
    "\n",
    "<a name='ex-3'></a>\n",
    "### Sigmoid\n",
    "$sigmoid(z) = \\frac{1}{1 + e^{-z}}$ for $z = w^T x + b$ to make predictions on the final output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "239ab1cf1028b721fd14f31b8103c40d",
     "grade": false,
     "grade_id": "cell-520521c430352f3b",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    Compute the sigmoid of z\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    z : array\n",
    "        A scalar or numpy array of any size.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    s : array\n",
    "        sigmoid(z)\n",
    "    \"\"\"\n",
    "    s = 1 / (1 + np.exp(-z))\n",
    "    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 - Initializing parameters\n",
    "\n",
    "The function below is used to initialize w (weights) as a vector of zeros and b as the bias term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c4a37e375a85ddab7274a33abf46bb7c",
     "grade": false,
     "grade_id": "cell-befa9335e479864e",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def initialize_with_zeros(n):\n",
    "    \"\"\"\n",
    "    This function creates a vector of zeros of shape (n, 1) for w and initializes b to 0.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    n : int\n",
    "        size of the w vector\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    w : array\n",
    "        initialized vector of shape (n, 1)\n",
    "    b : float\n",
    "        initialized scalar of type float (bias term)\n",
    "    \"\"\"\n",
    "    w = np.zeros((n, 1))\n",
    "    b = 0.\n",
    "\n",
    "    return w, b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 4.3 - Forward and Backward propagation\n",
    "\n",
    "Now that your parameters are initialized, we can do the \"forward\" and \"backward\" propagation steps for learning the parameters.\n",
    "\n",
    "Forward Propagation:\n",
    "- Computing $A = \\sigma(w^T X + b) = (a^{(1)}, a^{(2)}, ..., a^{(m-1)}, a^{(m)})$\n",
    "- Calculating the cost function: $J = -\\frac{1}{m}\\sum_{i=1}^{m}(y^{(i)}\\log(a^{(i)})+(1-y^{(i)})\\log(1-a^{(i)}))$\n",
    "\n",
    "Backward propagation to calculate derivatives (gradients): \n",
    "\n",
    "$$ \\frac{\\partial J}{\\partial w} = \\frac{1}{m}X(A-Y)^T$$\n",
    "$$ \\frac{\\partial J}{\\partial b} = \\frac{1}{m} \\sum_{i=1}^m (a^{(i)}-y^{(i)})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2ff9081e51809aef5e93bc1c21dc9b7b",
     "grade": false,
     "grade_id": "cell-11af17e28077b3d3",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def propagate(w, b, X, Y):\n",
    "    \"\"\"\n",
    "    Implement the cost function and its gradient for the propagation explained above\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    w : vector - weights, a numpy array of size (num_pixel * num_pixel * 3, 1)\n",
    "    b : int - bias, a scalar\n",
    "    X : 2-d array - data of size (num_pixel * num_pixel * 3, number of examples)\n",
    "    Y : array - true label of size (1, number of examples)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    cost : negative log-likelihood cost for logistic regression\n",
    "    dw : gradient of the loss with respect to w, thus same shape as w\n",
    "    db : gradient of the loss with respect to b, thus same shape as b\n",
    "    \"\"\"\n",
    "\n",
    "    m = X.shape[1]\n",
    "\n",
    "    # FORWARD PROPAGATION\n",
    "    A = sigmoid(np.dot(w.T, X) + b)  # compute activation\n",
    "    cost = (-1 / m) * np.sum(Y * np.log(A) + (1 - Y) * np.log(1 - A))  # calculate cost\n",
    "    cost = np.squeeze(np.array(cost))\n",
    "\n",
    "    # BACKWARD PROPAGATION (TO FIND GRAD)\n",
    "    dw = 1 / m * np.dot(X, (A - Y).T)\n",
    "    db = 1 / m * np.sum(A - Y)\n",
    "\n",
    "    grads = {\"dw\": dw, \"db\": db}\n",
    "\n",
    "    return grads, cost\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 4.4 - Optimization\n",
    "Using an optimize function we want to update the parameters using gradient descent. The goal is to learn $w$ and $b$ by minimizing the cost function $J$. For a parameter $\\theta$, the update rule is $ \\theta = \\theta - \\alpha \\text{ } d\\theta$, where $\\alpha$ is the learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "49d9b4c1a780bf141c8eb48e06cbb494",
     "grade": false,
     "grade_id": "cell-616d6883e807448d",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def optimize(w, b, X, Y, n_iter=100, alpha=0.009, verbose=False):\n",
    "    \"\"\"\n",
    "    This function optimizes w and b by running a gradient descent algorithm\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    w : vector - weights, a numpy array of size (num_pixel * num_pixel * 3, 1)\n",
    "    b : int - bias, a scalar\n",
    "    X : 2-d array - data of size (num_pixel * num_pixel * 3, number of examples)\n",
    "    Y : array - true label of size (1, number of examples)\n",
    "    n_iter : number of iterations of the optimization loop\n",
    "    alpha : learning rate of the gradient descent update rule\n",
    "    verbose : True to print the loss every 100 steps\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    params : dictionary containing the weights w and bias b\n",
    "    grads : dictionary containing the gradients of the weights and bias with respect to the cost function\n",
    "    costs : list of all the costs computed during the optimization, this will be used to plot the learning curve.\n",
    "    \"\"\"\n",
    "    \n",
    "    w = copy.deepcopy(w)\n",
    "    b = copy.deepcopy(b)\n",
    "    \n",
    "    costs = []\n",
    "    \n",
    "    for i in range(n_iter):\n",
    "        # Cost and gradient calculation \n",
    "        grads, cost = propagate(w, b, X, Y)\n",
    "\n",
    "        dw = grads[\"dw\"]\n",
    "        db = grads[\"db\"]\n",
    "        \n",
    "        # update weight and bias\n",
    "        w = w - alpha * grads['dw']\n",
    "        b = b - alpha * grads['db']\n",
    "           \n",
    "        # Record the costs\n",
    "        if i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "            if verbose:\n",
    "                print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "    \n",
    "    params = {\"w\": w,\n",
    "              \"b\": b}\n",
    "    \n",
    "    grads = {\"dw\": dw,\n",
    "             \"db\": db}\n",
    "    \n",
    "    return params, grads, costs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 4.5 - Prediction\n",
    "There are two steps to computing predictions:\n",
    "\n",
    "1. Calculate $\\hat{Y} = A = \\sigma(w^T X + b)$\n",
    "\n",
    "2. Convert the entries of a into 0 (if activation <= 0.5) or 1 (if activation > 0.5), stores the predictions in a vector `Y_prediction`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e56419b97ebf382a8f93ac2873988887",
     "grade": false,
     "grade_id": "cell-d6f924f49c51dc2f",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def predict(w, b, X):\n",
    "    '''\n",
    "    Predict whether the label is 0 or 1 using learned logistic regression parameters (w, b)\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    w : vector - weights, a numpy array of size (num_pixel * num_pixel * 3, 1)\n",
    "    b : int - bias, a scalar\n",
    "    X : 2-d array - data of size (num_pixel * num_pixel * 3, number of examples)\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Y_prediction : array containing all predictions (0/1) for the examples in X\n",
    "    '''\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    Y_prediction = np.zeros((1, m))\n",
    "    w = w.reshape(X.shape[0], 1)\n",
    "    \n",
    "    # Compute vector A predicting the probabilities\n",
    "    A = sigmoid(np.dot(w.T, X) + b)\n",
    "    \n",
    "    for i in range(A.shape[1]):\n",
    "        \n",
    "        # Convert probabilities A[0,i] to actual predictions\n",
    "        if A[0, i] > 0.5:\n",
    "            Y_prediction[0, i] = 1\n",
    "        else:\n",
    "            Y_prediction[0, i] = 0\n",
    "    \n",
    "    return Y_prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 - All functions into a model ##\n",
    "\n",
    "Implementing the model function using the following notation:\n",
    "    - Y_prediction_test for predictions on the test set\n",
    "    - Y_prediction_train for predictions on the train set\n",
    "    - parameters, grads, costs for the outputs of optimize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f23cca6cfb750397e5d2ac44977e2c2a",
     "grade": false,
     "grade_id": "cell-6dcba5967c4cbf8c",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def model(X_train, Y_train, X_test, Y_test, n_iter=2000, alpha=0.5, verbose=False):\n",
    "    \"\"\"\n",
    "    Builds the logistic regression model by calling the helper functions\n",
    " \n",
    "    Parameters\n",
    "    ----------\n",
    "    X_train : numpy array\n",
    "        training set of shape (num_pixel * num_pixel * 3, m_train)\n",
    "    Y_train : numpy array\n",
    "        training labels of shape (1, m_train)\n",
    "    X_test : numpy array\n",
    "        test set of shape (num_pixel * num_pixel * 3, m_test)\n",
    "    Y_test : numpy array\n",
    "        test labels of shape (1, m_test)\n",
    "    n_iter : int \n",
    "        number of iterations of the optimization loop\n",
    "    alpha : float\n",
    "        learning rate of the gradient descent update rule\n",
    "    verbose : Boolean\n",
    "        True to print the loss every 100 steps\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    d : dictionary \n",
    "        containing information about the model.\n",
    "    \"\"\"\n",
    "    \n",
    "    # initialize parameters with zeros \n",
    "    w, b = initialize_with_zeros(X_train.shape[0])\n",
    "    \n",
    "    # Gradient descent \n",
    "    params, grads, costs = optimize(w, b, X_train, Y_train, n_iter=n_iter, alpha=alpha, verbose=verbose)\n",
    "    \n",
    "    # Retrieve parameters w and b from dictionary \"params\"\n",
    "    w = params[\"w\"]\n",
    "    b = params[\"b\"]\n",
    "    \n",
    "    # Predict test/train set examples (≈ 2 lines of code)\n",
    "    Y_prediction_test = predict(w, b, X_test)\n",
    "    Y_prediction_train = predict(w, b, X_train)\n",
    "\n",
    "    # Print train/test Errors\n",
    "    if verbose:\n",
    "        print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100))\n",
    "        print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100))\n",
    "\n",
    "    \n",
    "    d = {\"costs\": costs,\n",
    "         \"Y_prediction_test\": Y_prediction_test, \n",
    "         \"Y_prediction_train\" : Y_prediction_train, \n",
    "         \"w\" : w, \n",
    "         \"b\" : b,\n",
    "         \"alpha\" : alpha,\n",
    "         \"n_iter\": n_iter}\n",
    "    \n",
    "    return d"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
